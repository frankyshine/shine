{
	"name": "SQLAnalysis_notes",
	"properties": {
		"folder": {
			"name": "Spark - Lake Database/Python"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "TestSparkPool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "1d1e2bd8-6e8b-424b-a4c5-ce99af3e743b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/53b2e7eb-9e0b-45c0-8fa3-b6279c1b186e/resourceGroups/msdnCertification/providers/Microsoft.Synapse/workspaces/msdncertsynapsewrkspace/bigDataPools/TestSparkPool",
				"name": "TestSparkPool",
				"type": "Spark",
				"endpoint": "https://msdncertsynapsewrkspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/TestSparkPool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# SUMMARY\r\n",
					"\r\n",
					"## Using SQL expressions in Spark\r\n",
					"The Dataframe API is part of a Spark library named Spark SQL, which enables data analysts to use SQL expressions to query and manipulate data.\r\n",
					"\r\n",
					"1) As such SQL style analyses can be preformed directly on Dataframes as below;\r\n",
					"\r\n",
					"        bikes_df = df.select(\"ProductName\", \"ListPrice\").where((df[\"Category\"]==\"Mountain Bikes\") | (df[\"Category\"]==\"Road Bikes\"))\r\n",
					"\r\n",
					"        display(bikes_df)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"2) However for a full ANSI SQL experience Spark provide a way to create relational data objects, namely Databses, temporaryviews, views, Tables etc.\r\n",
					"\r\n",
					"Creating database objects in the Spark catalog\r\n",
					"The Spark catalog is a metastore for relational data objects such as views and tables. \r\n",
					"\r\n",
					"once created, The Spark runtime can use the catalog to seamlessly integrate code written in any Spark-supported language with SQL expressions that may be more natural to some data analysts or developers.\r\n",
					"\r\n",
					"this can be achieved by:\r\n",
					"\r\n",
					"2.1)  using the Spark SQL API to embed SQL expressions in Spark code as below;\r\n",
					"\r\n",
					"    bikes_df = spark.sql(\"SELECT ProductID,     \r\n",
					"    ProductName, ListPrice FROM products \\\r\n",
					"WHERE Category IN ('Mountain Bikes', 'Road Bikes')\")\r\n",
					"\r\n",
					"For example, the PySpark code above uses a SQL query to return data from the products table/view created in the catalog as a dataframe.\r\n",
					"\r\n",
					"2.2) In a notebook, you can also use the %%sql magic to run Pure ANSI SQL code that queries objects in the catalog, like this:\r\n",
					"\r\n",
					"    %%sql\r\n",
					"    SELECT Category, COUNT(ProductID) AS ProductCount\r\n",
					"    FROM products\r\n",
					"    GROUP BY Category\r\n",
					"    ORDER BY Category\r\n",
					"\r\n",
					"## Spark Catalog in central metastore\r\n",
					"Spark manages all the complexities of creating and managing Databases, views and tables, both in memory and on disk.\r\n",
					"Associated with each Relational object in Spark is its relevant metadata (schema, description, table name, database name, column names, partitions, physical location where the actual data resides, etc.) \r\n",
					"Metadata is store in a central metastore. \r\n",
					"\r\n",
					"Instead of having a separate metastore for Spark tables, Spark by default uses the Apache Hive metastore, located at /user/hive/warehouse.\r\n",
					"\r\n",
					"Note: However, you may change the default location by setting the Spark config variable spark.sql.warehouse.dir to another location, which can be set to a local or external distributed storage\r\n",
					"\r\n",
					"## Catalog Managed Vs Unmanaged Tables\r\n",
					"Tables are main objects in \r\n",
					"all other objcets are autmatically managed\r\n",
					"\r\n",
					"managed (catalog) tables:\r\n",
					" For a managed table, Spark manages both the metadata and the data in the file store. This could be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob. \r\n",
					"\r\n",
					"unmanaged (catelog) table:\r\n",
					"For an unmanaged table, Spark only manages the metadata, while you manage the data yourself in an external data source.\r\n",
					"\r\n",
					"\r\n",
					"With a managed table, because Spark manages everything, a SQL command such as DROP TABLE table_name deletes both the metadata and the data. With an unmanaged table, the same command will delete only the metadata, not the actual data\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"## Creating Catalog Objects\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"#### Creating SQL Databases for managed Tables\r\n",
					"\r\n",
					"Tables reside within a database. By default, Spark creates tables under the default database. To create your own database name, you can issue a SQL command from your Spark application or notebook. \r\n",
					"To begin, you create a database and tell Spark we want to use that database:\r\n",
					"\r\n",
					"    // In Scala/Python\r\n",
					"    spark.sql(\"CREATE DATABASE learn_spark_db\")\r\n",
					"    spark.sql(\"USE learn_spark_db\")\r\n",
					"\r\n",
					"From this point, any commands we issue in our application to create tables will result in the tables being created in this database and residing under the database name learn_spark_db.\r\n",
					"\r\n",
					"\r\n",
					"#### Creating Managed tables\r\n",
					"To create a managed table within the database learn_spark_db, you can issue a SQL query like the following:\r\n",
					"\r\n",
					"// In Scala/Python\r\n",
					"\r\n",
					"    spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,  \r\n",
					"    distance INT, origin STRING, destination STRING)\")\r\n",
					"\r\n",
					"\r\n",
					"You can do the same thing using the DataFrame API like this:\r\n",
					"\r\n",
					"    # In Python\r\n",
					"    # Path to our US flight delays CSV file \r\n",
					"    csv_file = \"/databricks-datasets/learning-spark-v2/flights/departuredelays.csv\"\r\n",
					"\r\n",
					"    # Schema as defined in the preceding example\r\n",
					"    schema=\"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\r\n",
					"    flights_df = spark.read.csv(csv_file, schema=schema)\r\n",
					"    flights_df.write.saveAsTable(\"managed_us_delay_flights_tbl\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"##### Creating unmanaged tables \r\n",
					"\r\n",
					"Creating an unmanaged table\r\n",
					"By contrast, you can create unmanaged tables from your own data sources—say, Parquet, CSV, or JSON files stored in a file store accessible to your Spark application.\r\n",
					"\r\n",
					"To create an unmanaged table from a data source such as a CSV file, in SQL use:\r\n",
					"\r\n",
					"    spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \r\n",
					"    distance INT, origin STRING, destination STRING) \r\n",
					"    USING csv OPTIONS (PATH \r\n",
					"    '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")\r\n",
					"\r\n",
					"And within the DataFrame API use:\r\n",
					"\r\n",
					"    (flights_df\r\n",
					"    .write\r\n",
					"    .option(\"path\", \"/tmp/data/us_flights_delay\")\r\n",
					"    .saveAsTable(\"us_delay_flights_tbl\"))\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"#### Creating Views\r\n",
					"In addition to creating tables, Spark can create views on top of existing tables. Views can be global (visible across all SparkSessions on a given cluster) or session-scoped (visible only to a single SparkSession), and they are temporary: they disappear after your Spark application terminates.\r\n",
					"\r\n",
					"Creating views has a similar syntax to creating tables within a database. Once you create a view, you can query it as you would a table. The difference between a view and a table is that views don’t actually hold the data; tables persist after your Spark application terminates, but views disappear.\r\n",
					"\r\n",
					"You can create a view from an existing table using SQL. For example, if you wish to work on only the subset of the US flight delays data set with origin airports of New York (JFK) and San Francisco (SFO), the following queries will create global temporary and temporary views consisting of just that slice of the table:\r\n",
					"\r\n",
					"-- In SQL\r\n",
					"    CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\r\n",
					"    SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE \r\n",
					"    origin = 'SFO';\r\n",
					"\r\n",
					"    CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\r\n",
					"    SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE \r\n",
					"    origin = 'JFK'\r\n",
					"\r\n",
					"You can accomplish the same thing with the DataFrame API as follows:\r\n",
					"\r\n",
					"# In Python\r\n",
					"    df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM \r\n",
					"    us_delay_flights_tbl WHERE origin = 'SFO'\")\r\n",
					"    \r\n",
					"    df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM \r\n",
					"    us_delay_flights_tbl WHERE origin = 'JFK'\")\r\n",
					"\r\n",
					"#### Create a temporary and global temporary view\r\n",
					"\r\n",
					"    df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\r\n",
					"    df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")\r\n",
					"\r\n",
					"Once you’ve created these views, you can issue queries against them just as you would against a table. Keep in mind that when accessing a global temporary view you must use the prefix global_temp.<view_name>, because Spark creates global temporary views in a global temporary database called global_temp. For example:\r\n",
					"\r\n",
					"-- In SQL \r\n",
					"    SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\r\n",
					"    By contrast, you can access the normal temporary view without the global_temp prefix:\r\n",
					"\r\n",
					"-- In SQL \r\n",
					"    SELECT * FROM us_origin_airport_JFK_tmp_view\r\n",
					"\r\n",
					"// In Scala/Python\r\n",
					"    spark.read.table(\"us_origin_airport_JFK_tmp_view\")\r\n",
					"    // Or\r\n",
					"    spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\")\r\n",
					"    You can also drop a view just like you would a table:\r\n",
					"\r\n",
					"-- In SQL\r\n",
					"    DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\r\n",
					"    DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\r\n",
					"\r\n",
					"// In Scala/Python\r\n",
					"    spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\r\n",
					"    spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")\r\n",
					"\r\n",
					"\r\n",
					"#### Temporary views versus global temporary views\r\n",
					"The difference between temporary and global temporary views being subtle, it can be a source of mild confusion among developers new to Spark. A temporary view is tied to a single SparkSession within a Spark application. In contrast, a global temporary view is visible across multiple SparkSessions within a Spark application. Yes, you can create multiple SparkSessions within a single Spark application—this can be handy, for example, in cases where you want to access (and combine) data from two different SparkSessions that don’t share the same Hive metastore configurations.\r\n",
					"\r\n",
					"#### Viewing the Metadata\r\n",
					"As mentioned previously, Spark manages the metadata associated with each managed or unmanaged table. This is captured in the Catalog, a high-level abstraction in Spark SQL for storing metadata. The Catalog’s functionality was expanded in Spark 2.x with new public methods enabling you to examine the metadata associated with your databases, tables, and views. Spark 3.0 extends it to use external catalog (which we briefly discuss in Chapter 12).\r\n",
					"\r\n",
					"For example, within a Spark application, after creating the SparkSession variable spark, you can access all the stored metadata through methods like these:\r\n",
					"\r\n",
					"    // In Scala/Python\r\n",
					"    spark.catalog.listDatabases()\r\n",
					"    spark.catalog.listTables()\r\n",
					"    spark.catalog.listColumns(\"us_delay_flights_tbl\")\r\n",
					"\r\n",
					"Import the notebook from the book’s GitHub repo and give it a try.\r\n",
					"\r\n",
					"#### Caching SQL Tables\r\n",
					"Although we will discuss table caching strategies in the next chapter, it’s worth mentioning here that, like DataFrames, you can cache and uncache SQL tables and views. In Spark 3.0, in addition to other options, you can specify a table as LAZY, meaning that it should only be cached when it is first used instead of immediately:\r\n",
					"\r\n",
					"    -- In SQL\r\n",
					"    CACHE [LAZY] TABLE <table-name>\r\n",
					"    UNCACHE TABLE <table-name>\r\n",
					"\r\n",
					"#### Reading Tables into DataFrames\r\n",
					"Often, data engineers build data pipelines as part of their regular data ingestion and ETL processes. They populate Spark SQL databases and tables with cleansed data for consumption by applications downstream.\r\n",
					"\r\n",
					"Let’s assume you have an existing database, learn_spark_db, and table, us_delay_flights_tbl, ready for use. Instead of reading from an external JSON file, you can simply use SQL to query the table and assign the returned result to a DataFrame:\r\n",
					"\r\n",
					"    # In Python\r\n",
					"    us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\r\n",
					"    us_flights_df2 = spark.table(\"us_delay_flights_tbl\")\r\n",
					"\r\n",
					"Now you have a cleansed DataFrame read from an existing Spark SQL table. You can also read data in other formats using Spark’s built-in data sources, giving you the flexibility to interact with various common file formats.\r\n",
					"\r\n",
					""
				]
			}
		]
	}
}