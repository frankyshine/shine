{
	"name": "3_WWI_SQLSvrlessLogicalDW_loading",
	"properties": {
		"description": "Incremental loading of Fact and SCD",
		"folder": {
			"name": "Synapse SQL/Serverless SQL Pools"
		},
		"content": {
			"query": "\n/*\n\nNew and Changed Data to Load\nFrom <https://www.serverlesssql.com/creating-a-logical-data-warehouse-with-synapse-serverless-sql-part-3-of-3-incremental-fact-loading-and-slowly-changing-dimensions/> \n\nNew and Changed Data to Load\nIn our data warehousing scenario we have Sales Order data that is being written to the Data Lake in a \nYYYY-MM-DD dated folder structure, each days sales are saved to a separate folder. We now have new\n Sales Orders and Sales Order Lines CSV files to load for 2 days. There has also been an update to \n an existing Supplier to change their Supplier Category, this change we must track over time. \n \n We will reflect these changes to the Supplier as a Type 2 Slowly Changing Dimension in that we will \n create a new Supplier dimension row whilst still keeping the old value for historical purposes. \n There has also been a new Supplier added which must be added to the dimension.\n\nNew Sales Data and New and Changed Supplier data can be found on GitHub here.\n• The Sales data needs placing in the following new folders. YYYY-MM-DD must be replaced with the \ndate of the data.\n• /sourcedatapartitionsalesorder/YYYY-MM-DD/\n• /sourcedatapartitionsalesorderline/YYYY-MM-DD/\n• The Supplier data needs placing in /sourcedatasystem/changeddata/2021-06-22/Purchasing_Suppliers/\n*/\n\n\n/*\nManual CETAS to load new Sales Order data\n*******************************************\nWe’ll start by loading the 18th April CSV data by specifying a date folder named as YYYY-MM-DD in the\n LOCATION parameter in the CETAS statement and using the FilePathDate column in the source View to\n  only select the data we need to load. The FilePathDate column in the View is the result of the \n  filepath() function which can be used as a filter to only select/scan the requested folder. \n  This reduces the amount of data processed as only the required folder and therefore data within \n  the folder is scanned.\n\nAs with Part 2, the External Table (CETAS) only exists to load data to the Azure Data Lake folder and can\n be dropped after the load process has completed. We use Views to query the loaded Fact data.\n*/\n\nCREATE EXTERNAL TABLE STG.FactSales\nWITH \n(\n  LOCATION = 'conformed/facts/factsales/incremental/2021-04-18',\n  DATA_SOURCE = ExternalDataSourceDataLake,\n  FILE_FORMAT = SynapseParquetFormat\n) \nAS\nSELECT  \n  --Surrogate Keys \n    DC.CustomerKey,\n    CAST(FORMAT(SO.OrderDate,'yyyyMMdd') AS INT) as OrderDateKey,\n    DSI.StockItemKey,\n    DS.SupplierKey,\n    --Degenerate Dimensions\n    CAST(SO.OrderID AS INT) AS OrderID,\n    CAST(SOL.OrderLineID AS INT) AS OrderLineID,  \n    --Measure\n    CAST(SOL.Quantity AS INT) AS SalesOrderQuantity, \n    CAST(SOL.UnitPrice AS DECIMAL(18,2)) AS SalesOrderUnitPrice\nFROM LDW.vwSalesOrdersLines SOL\nINNER JOIN LDW.vwSalesOrders SO ON SOL.OrderID = SO.OrderID\nLEFT JOIN LDW.vwDimCustomer DC ON DC.CustomerID = SO.CustomerID\nLEFT JOIN LDW.vwDimStockItem DSI ON DSI.StockItemID = SOL.StockItemID\nLEFT JOIN LDW.vwStockItems SI ON SI.StockItemID = DSI.StockItemID\nLEFT JOIN LDW.vwDimSupplier DS ON DS.SupplierID = SI.SupplierID\nWHERE SOL.FilePathDate = '2021-04-18' AND SO.FilePathDate = '2021-04-18';\n\n\n/*\nDynamic SQL with a Stored Procedure to load Sales Data\nAlthough the SQL code above will load new data, we cannot parameterise the LOCATION value to change \nthe folder where data is stored. To transform the above code into a more flexible process, we can\nuse dynamic SQL to create the LOCATION parameter and also select the source data using a single \n date input parameter. \n \n The SQL code below will create a Stored Procedure which takes a single Date\nparameter as input, selects source data based on this date parameter, then writes the data out to\n a destination folder using the same date parameter.\n*/\n\n\nCREATE PROCEDURE STG.FactSalesLoad @ProcessDate DATE\nWITH ENCRYPTION\nAS\n\nBEGIN\n\nDECLARE @location varchar(100)\n\nIF OBJECT_ID('STG.FactSales') IS NOT NULL \n  DROP EXTERNAL TABLE STG.FactSales\n\nSET @location = CONCAT('conformed/facts/factsales/incremental/',FORMAT (@ProcessDate, 'yyyy/MM/dd') )\n\nDECLARE @CreateExternalTableString NVARCHAR(2000)\n\nSET @CreateExternalTableString = \n'CREATE EXTERNAL TABLE STG.FactSales\nWITH \n(\n  LOCATION = ''' + @location + ''',                                      \n  DATA_SOURCE = ExternalDataSourceDataLake,\n  FILE_FORMAT = SynapseParquetFormat\n)\nAS\nSELECT  \n--Surrogate Keys \nDC.CustomerKey,\nCAST(FORMAT(SO.OrderDate,''yyyyMMdd'') AS INT) as OrderDateKey,\nDSI.StockItemKey,\nDS.SupplierKey,\n--Degenerate Dimensions\nCAST(SO.OrderID AS INT) AS OrderID,\nCAST(SOL.OrderLineID AS INT) AS OrderLineID,  \n--Measure\nCAST(SOL.Quantity AS INT) AS SalesOrderQuantity, \nCAST(SOL.UnitPrice AS DECIMAL(18,2)) AS SalesOrderUnitPrice\nFROM LDW.vwSalesOrdersLines SOL\nINNER JOIN LDW.vwSalesOrders SO ON SOL.OrderID = SO.OrderID\nLEFT JOIN LDW.vwDimCustomer DC ON DC.CustomerID = SO.CustomerID\nLEFT JOIN LDW.vwDimStockItem DSI ON DSI.StockItemID = SOL.StockItemID\nLEFT JOIN LDW.vwStockItems SI ON SI.StockItemID = DSI.StockItemID\nLEFT JOIN LDW.vwDimSupplier DS ON DS.SupplierID = SI.SupplierID\nWHERE SOL.FilePathDate = ''' + CAST(@ProcessDate AS CHAR(10)) + '''  AND SO.FilePathDate = ''' + CAST(@ProcessDate AS CHAR(10)) + ''''\n\nEXEC sp_executesql @CreateExternalTableString\n\nEND;\n\n-- Now we can run the Proc which will load the source CSV data into the required destination date folder.\n\nEXEC STG.FactSalesLoad '2021-04-19';\n\n-- Before we continue, let’s run the Stored Procedure again with the same date parameter…\n\nEXEC STG.FactSalesLoad '2021-04-19';\n\n/*\nWe’ll get an error stating that the “location already exists” even though we have dropped \nthe External Table in the Stored Procedure. The current DROP EXTERNAL TABLE command will drop \nthe table from the database but the underlying folder and files in the Data Lake will not be deleted.\n*/\n\n\n\n/*\nSlowly Changing Dimensions\n*************************************\nWhen we look at loading changed data for dimensions that must be tracked over time, we have to be \naware that Serverless SQL Pools currently does not support updating data in the Data Lake, it is \nan append-only process in that files can be added to the underlying storage but we cannot run \nSQL to change existing data. However, we can load new and changed dimension data into new destination \nfolders under the root dimension folder.\n\nSelect and Load the Supplier Data Changes\nThe SQL code below will create a View which targets selecting changed data for Suppliers.\n*/\n\n\nCREATE VIEW LDW.vwIncrementalSuppliers\nAS\nSELECT fct.*,\nfct.filepath(1) AS FilePathDate\nFROM \nOPENROWSET \n(\n    BULK 'sourcedatasystem/ChangedData/*/Purchasing_Suppliers/*.csv',\n    DATA_SOURCE = 'ExternalDataSourceDataLake',\n    FORMAT = 'CSV',\n    PARSER_VERSION = '2.0',\n    HEADER_ROW = TRUE,\n    FIELDTERMINATOR ='|'\n) AS fct\n\n/*\nThe SQL code below will now write the new and changed data out to a sub-folder /02 in the current \nSupplier dimension folder. This can be amended to use dynamic SQL as seen in the Sales Order process.\nWe first select the maximum surrogate key from the current dimension data and use this to continue\nthe sequence when writing the changed and new data. Within the CSV file is a date column which \nindicates when the source data changed, we can use this as our ValidFrom value.\n*/\n\nDECLARE @MaxKey TINYINT\nSELECT @MaxKey = MAX(SupplierKey) FROM LDW.vwDimSupplier\n\nIF OBJECT_ID('STG.DimSupplier') IS NOT NULL \n    DROP EXTERNAL TABLE STG.DimSupplier;\n\nCREATE EXTERNAL TABLE STG.DimSupplier\nWITH \n(\n  LOCATION = 'conformed/dimensions/dimsupplier/02',\n  DATA_SOURCE = ExternalDataSourceDataLake,\n  FILE_FORMAT = SynapseParquetFormat\n) \nAS\nSELECT CAST(ROW_NUMBER() OVER(ORDER BY S.SupplierID) AS TINYINT) + @MaxKey AS SupplierKey,\nS.SupplierID,\nS.SupplierName,\nSC.SupplierCategoryName,\nCAST(S.ValidFrom AS DATE) AS ValidFromDate\nFROM LDW.vwIncrementalSuppliers S\nLEFT JOIN LDW.vwSupplierCategories SC ON SC.SupplierCategoryID = S.SupplierCategoryID\nWHERE S.FilePathDate = '2021-06-22'\nORDER BY S.SupplierID;\n\n\n/*\nSelecting data from the Supplier Dimension\n*****************************************\nIf we now query the existing View to select data from the Supplier dimension, we get all the existing data, the changed data, and the new data. However, we are missing vital columns which are required to flag the date range validity of a dimension row.\n*/\n\nSELECT * \nFROM LDW.vwDimSupplier\nWHERE SupplierID IN (5,14)\nORDER BY SupplierID;\n\n\n--In the image above we see the result which shows that SupplierID 5 has changed Supplier Category.\n\n\n/*\nCreate View to construct a complete SCD Type 2 Dimension\nWe can use the single ValidFrom date to calculate the ValidTo and also calculate the ActiveMember \nflag for each dimension row. We use the LEAD function and partition by the SupplierID \n(source system business key) to generate contiguous date ranges. Please note that we can also use\n datetime values and change the DATEADD accordingly.\n*/\n\n\nCREATE VIEW LDW.vwDimSupplierSCD\nAS\nSELECT SupplierKey,\n        SupplierID,\n        SupplierName,\n        SupplierCategoryName,\n        ValidFromDate,\n        ISNULL(DATEADD(DAY,-1,LEAD(ValidFromDate,1) OVER (PARTITION BY SupplierID ORDER BY SupplierKey)),'2099-01-01') AS ValidToDate,\n        CASE ROW_NUMBER() OVER(PARTITION BY SupplierID ORDER BY SupplierKey DESC) WHEN 1 THEN 'Y' ELSE 'N' END AS ActiveMember\nFROM LDW.vwDimSupplier\n\n\n/*\nNow when we select from the new Dimension View we are able to see date ranges and which row is \nthe current active member.\n*/\n\nSELECT * \nFROM LDW.vwDimSupplierSCD\nWHERE SupplierID IN (1,5,14)\nORDER BY SupplierID,SupplierKey\n\n\n/*\nAmend Fact Loading Stored Procedure\n*****************************\nWe can now use the SCD version of the Supplier dimension view LDW.vwDimSupplierSCD and use the\n Sales Order date to select the correct dimension value at the time the Sales Order was created. \n We have added an additional JOIN to the new dimension view using the Sales Order date and the \n Supplier ValidFrom and ValidTo dates.\n*/\n\n\nCREATE PROCEDURE STG.FactSalesLoadCSD @ProcessDate DATE\nWITH ENCRYPTION\nAS\n\nBEGIN\n\nDECLARE @location varchar(100)\n\nIF OBJECT_ID('STG.FactSales') IS NOT NULL \n  DROP EXTERNAL TABLE STG.FactSales\n\nSET @location = CONCAT('conformed/facts/factsales/incremental/',FORMAT (@ProcessDate, 'yyyy/MM/dd') )\n\nDECLARE @CreateExternalTableString NVARCHAR(2000)\n\nSET @CreateExternalTableString = \n'CREATE EXTERNAL TABLE STG.FactSales\nWITH \n(\n  LOCATION = ''' + @location + ''',                                      \n  DATA_SOURCE = ExternalDataSourceDataLake,\n  FILE_FORMAT = SynapseParquetFormat\n)\nAS\nSELECT  \n--Surrogate Keys \nDC.CustomerKey,\nCAST(FORMAT(SO.OrderDate,''yyyyMMdd'') AS INT) as OrderDateKey,\nDSI.StockItemKey,\nDS.SupplierKey,\n--Degenerate Dimensions\nCAST(SO.OrderID AS INT) AS OrderID,\nCAST(SOL.OrderLineID AS INT) AS OrderLineID,  \n--Measure\nCAST(SOL.Quantity AS INT) AS SalesOrderQuantity, \nCAST(SOL.UnitPrice AS DECIMAL(18,2)) AS SalesOrderUnitPrice\nFROM LDW.vwSalesOrdersLines SOL\nINNER JOIN LDW.vwSalesOrders SO ON SOL.OrderID = SO.OrderID\nLEFT JOIN LDW.vwDimCustomer DC ON DC.CustomerID = SO.CustomerID\nLEFT JOIN LDW.vwDimStockItem DSI ON DSI.StockItemID = SOL.StockItemID\nLEFT JOIN LDW.vwStockItems SI ON SI.StockItemID = DSI.StockItemID\nLEFT JOIN LDW.vwDimSupplierSCD  DS ON DS.SupplierID = SI.SupplierID \n                                AND SO.OrderDate BETWEEN DS.ValidFromDate AND DS.ValidToDate\nWHERE SOL.FilePathDate = ''' + CAST(@ProcessDate AS CHAR(10)) + '''  AND SO.FilePathDate = ''' + CAST(@ProcessDate AS CHAR(10)) + ''''\n\nEXEC sp_executesql @CreateExternalTableString\n\nEND\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "master",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}