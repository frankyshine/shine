{
	"name": "Create and use native external tables using SQL pools",
	"properties": {
		"folder": {
			"name": "Synapse SQL/Serverless SQL Pools"
		},
		"content": {
			"query": "\n/*\nCreate and use native external tables using SQL pools in Azure Synapse Analytics\n*******************************************************************************************\n\n\nPrerequisites\nExternal table on a file\nExternal table on a set of files\nExternal table on appendable files\n\nIn this section, you'll learn how to create and use native external tables in Synapse SQL pools. Native external tables have \nbetter performance when compared to external tables with TYPE=HADOOP in their external data source definition. This is because \nnative external tables use native code to access external data.\n\nExternal tables are useful when you want to control access to external data in Synapse SQL pool. External tables are also \nuseful if you want to use tools, such as Power BI, in conjunction with Synapse SQL pool. External tables can access two types \nof storage:\n\n1. Public storage where users access public storage files.\n2. Protected storage where users access storage files using SAS credential, Azure AD identity, or Managed Identity of Synapse workspace.\nNote\n\nIn dedicated SQL pools you can only use native external tables with a Parquet file type, and this feature is in public preview. \nIf you want to use generally available Parquet reader functionality in dedicated SQL pools, \nor you need to access CSV or ORC files, use Hadoop external tables. \nNative external tables are generally available in serverless SQL pools. Learn more about the differences between native \nand Hadoop external tables in Use external tables with Synapse SQL.\n\nThe following table lists the data formats supported:\n\nData format                     Serverless SQL pool\t    Dedicated SQL pool\n(Native external tables)\n-------------------------------------------------------------------------------\nParquet\t                            Yes (GA)\t        Yes (public preview)\nCSV\t                                Yes\t                No (Alternatively, use Hadoop external tables)\ndelta\t                            Yes\t                No\nSpark\t                            Yes\t                No\nDataverse\t                        Yes\t                No\nAzure Cosmos DB data formats \n(JSON, BSON etc.)\t               No                   No\n                                   (Alternatively,\n                                    create views)\t\n\n\n\nPrerequisites\nYour first step is to create a database where the tables will be created. Before creating a database scoped credential, the \ndatabase must have a master key to protect the credential. For more information on this, see CREATE MASTER KEY (Transact-SQL). \nThen create the following objects that are used in this sample:\n\n1. DATABASE SCOPED CREDENTIAL sqlondemand that enables access to SAS-protected \nhttps://sqlondemandstorage.blob.core.windows.net Azure storage account.\n\n*/\n\n        CREATE DATABASE SCOPED CREDENTIAL [sqlondemand]\n        WITH IDENTITY='SHARED ACCESS SIGNATURE',  \n        SECRET = 'sv=2018-03-28&ss=bf&srt=sco&sp=rl&st=2019-10-14T12%3A10%3A25Z&se=2061-12-31T12%3A10%3A00Z&sig=KlSU2ullCscyTS0An0nozEpo4tO5JAgGBvw%2FJX2lguw%3D'\n\n/*\n2. EXTERNAL DATA SOURCE sqlondemanddemo that references demo storage account protected with SAS key, \n    and EXTERNAL DATA SOURCE nyctlc that references publicly \n  available Azure storage account on location https://azureopendatastorage.blob.core.windows.net/nyctlc/.\n*/\n\nCREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (\n    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',\n    CREDENTIAL = sqlondemand\n);\nGO\n\nCREATE EXTERNAL DATA SOURCE nyctlc\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/');\nGO\n\nCREATE EXTERNAL DATA SOURCE DeltaLakeStorage\nWITH ( LOCATION = 'https://sqlondemandstorage.blob.core.windows.net/delta-lake/' );\n\n\n-- 3. File formats QuotedCSVWithHeaderFormat and ParquetFormat that describe CSV and parquet file types.\n\nCREATE EXTERNAL FILE FORMAT QuotedCsvWithHeaderFormat\nWITH (  \n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS ( FIELD_TERMINATOR = ',', STRING_DELIMITER = '\"', FIRST_ROW = 2   )\n);\nGO\n\nCREATE EXTERNAL FILE FORMAT ParquetFormat WITH (  FORMAT_TYPE = PARQUET );\nGO\n\nCREATE EXTERNAL FILE FORMAT DeltaLakeFormat WITH (  FORMAT_TYPE = DELTA );\nGO\n\n\n-- The queries in this article will be executed on your sample database and use these objects.\n\n/*\nExternal table on a file\n--------------------------\nYou can create external tables that access data on an Azure storage account that allows access to users with some \nAzure AD identity or SAS key. You can create external tables the same way you create regular SQL Server external tables.\n\nThe following query creates an external table that reads population.csv file from SynapseSQL demo Azure storage \naccount that is referenced using sqlondemanddemo data source and protected with database scoped credential called sqlondemand.\n\nData source and database scoped credential are created in setup script.\n\n*** Note\nChange the first line in the query, i.e., [mydbname], so you're using the database you created.\n*/\n\n\nUSE [mydbname];\nGO\nCREATE EXTERNAL TABLE populationExternalTable\n(\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n)\nWITH (\n    LOCATION = 'csv/population/population.csv',\n    DATA_SOURCE = sqlondemanddemo,\n    FILE_FORMAT = QuotedCSVWithHeaderFormat\n);\n\n/*\nNative CSV tables are currently available only in the serverless SQL pools.\n\nExternal table on a set of files\n-----------------------------------\nYou can create external tables that read data from a set of files placed on Azure storage:\n*/\n\n\nCREATE EXTERNAL TABLE Taxi (\n     vendor_id VARCHAR(100) COLLATE Latin1_General_BIN2, \n     pickup_datetime DATETIME2, \n     dropoff_datetime DATETIME2,\n     passenger_count INT,\n     trip_distance FLOAT,\n     fare_amount FLOAT,\n     tip_amount FLOAT,\n     tolls_amount FLOAT,\n     total_amount FLOAT\n) WITH (\n         LOCATION = 'yellow/puYear=*/puMonth=*/*.parquet',\n         DATA_SOURCE = nyctlc,\n         FILE_FORMAT = ParquetFormat\n);\n\n/*\nYou can specify the pattern that the files must satisfy in order to be referenced by the external table. The pattern is \nrequired only for Parquet and CSV tables. If you are using Delta Lake format, you need to specify just a root folder, \nand the external table will automatically find the pattern.\n\n ** Note\nThe table is created on partitioned folder structure, but you cannot leverage some partition elimination. If you want to \nget better performance by skipping the files that do not satisfy some criterion (like specific year or month in this case), \nuse views on external data.\n\nExternal table on appendable files\n-----------------------------------\nThe files that are referenced by an external table should not be changed while the query is running. In the long-running query, \nSQL pool may retry reads, read parts of the files, or even read the file multiple times. Changes of the file content would \ncause wrong results. Therefore, the SQL pool fails the query if detects that the modification time of any file is changed \nduring the query execution. In some scenarios you might want to create a table on the files that are constantly appended. \nTo avoid the query failures due to constantly appended files, you can specify that the external table should ignore \npotentially inconsistent reads using the TABLE_OPTIONS setting.\n*/\n\n\nCREATE EXTERNAL TABLE populationExternalTable\n(\n    [country_code] VARCHAR (5) COLLATE Latin1_General_BIN2,\n    [country_name] VARCHAR (100) COLLATE Latin1_General_BIN2,\n    [year] smallint,\n    [population] bigint\n)\nWITH (\n    LOCATION = 'csv/population/population.csv',\n    DATA_SOURCE = sqlondemanddemo,\n    FILE_FORMAT = QuotedCSVWithHeaderFormat,\n    TABLE_OPTIONS = N'{\"READ_OPTIONS\":[\"ALLOW_INCONSISTENT_READS\"]}'\n);\n\n\n/*\nThe ALLOW_INCONSISTENT_READS read option will disable file modification time check during the query lifecycle and read \nwhatever is available in the files that are referenced by the external table. In appendable files, the existing content \nis not updated, and only new rows are added. Therefore, the probability of wrong results is minimized compared to the \nupdateable files. This option might enable you to read the frequently appended files without handling the errors.\n\nThis option is available only in the external tables created on CSV file format.\n\n Note\nAs the option name implies, the creator of the table accepts a risk that the results might not be consistent. \nIn the appendable files, you might get incorrect results if you force multiple read of the underlying files by \nself-joining the table. In most of the \"classic\" queries, the external table will just ignore some rows that are \nappended while the query was running.\n\n\n\nDelta Lake external table\n--------------------------\nExternal tables can be created on top of a Delta Lake folder. The only difference between the external tables created on \na single file or a file set and the external tables created on a Delta Lake format is that in Delta Lake external table \nyou need to reference a folder containing the Delta Lake structure.\n\n\nECDC COVID-19 Delta Lake folder\nAn example of a table definition created on a Delta Lake folder is:\n*/\n\nCREATE EXTERNAL TABLE Covid (\n     date_rep date,\n     cases int,\n     geo_id varchar(6)\n) WITH (\n        LOCATION = 'covid', --> the root folder containing the Delta Lake files\n        data_source = DeltaLakeStorage,\n        FILE_FORMAT = DeltaLakeFormat\n);\n\n/*\nExternal tables cannot be created on a partitioned folder. Review the other known issues on Synapse serverless SQL \npool self-help page.\n\n\nDelta tables on partitioned folders\n------------------------------------\nExternal tables in serverless SQL pools do not support partitioning on Delta Lake format. Use Delta partitioned views instead of \ntables if you have partitioned Delta Lake data sets.\n\nImportant\nDo not create external tables on partitioned Delta Lake folders even if you see that they might work in some cases. \nUsing unsupported features like external tables on partitioned delta folders might cause issues or instability of the serverless pool. Azure support will not be able to resolve any issue if it is using tables on partitioned folders. You would be asked to transition to Delta partitioned views and rewrite your code to use only the supported feature before proceeding with issue resolution.\n\n\nUse an external table\n-------------------------\nYou can use external tables in your queries the same way you use them in SQL Server queries.\n\nThe following query demonstrates this using the population external table we created in previous section. \nIt returns country/region names with their population in 2019 in descending order.\n\n\n** Note\nChange the first line in the query, i.e., [mydbname], so you're using the database you created.\n*/\n\nUSE [mydbname];\nGO\n\nSELECT\n    country_name, population\nFROM populationExternalTable\nWHERE\n    [year] = 2019\nORDER BY\n    [population] DESC;\n\n/*\nPerformance of this query might vary depending on region. Your workspace might not be placed in the same region as the \nAzure storage accounts used in these samples. For production workloads, place your Synapse workspace and Azure storage \nin the same region.\n*/\n\n\n\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "master",
				"poolName": "Built-in"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}